# Shared-git-of-Security-and-Privacy-on-LLM

## Privacy

1. [LARGE LANGUAGE MODELS CAN BE GOOD PRIVACY PROTECTION LEARNERS](https://openreview.net/pdf?id=6JcbNMEFPw)

2. [BEYOND MEMORIZATION: VIOLATING PRIVACY VIA INFERENCE WITH LARGE LANGUAGE MODELS](https://openreview.net/attachment?id=kmn0BhQk7p&name=pdf)

3. [LAST ONE STANDING: A COMPARATIVE ANALYSIS OF SECURITY AND PRIVACY OF SOFT PROMPT TUNING, LORA, AND IN-CONTEXT LEARNING](https://openreview.net/attachment?id=3TAhlGaMKD&name=pdf)

## Red teaming

1. [Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://arxiv.org/pdf/2209.07858.pdf)

2. [Red Teaming Language Models with Language Models](https://arxiv.org/pdf/2202.03286.pdf)

3. [Attack Prompt Generation for Red Teaming and Defending Large Language Models](https://arxiv.org/pdf/2310.12505.pdf)

## Jailbreak

1. [“Do Anything Now”: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models](https://arxiv.org/pdf/2308.03825.pdf)

2. [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/pdf/2310.03684.pdf)


